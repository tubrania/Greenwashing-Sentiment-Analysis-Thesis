{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Google Colab Drive Mounting and Directory Change:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XeBTKg3d4ZXX"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change directory to the project directory\n","\n","import os\n","os.chdir('/content/drive/MyDrive/')"]},{"cell_type":"markdown","metadata":{},"source":["### Installation of Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lCBwhw2SBOFS"},"outputs":[],"source":["!pip install accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzSUGX3aAwKX"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWScyM_69BtD"},"outputs":[],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDoXlkLB9V2F"},"outputs":[],"source":["!pip install evaluate\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data Processing and Experiment Automation Functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlLtYtCG42xc"},"outputs":[],"source":["import os.path as osp\n","import re\n","import datetime\n","from collections import Counter\n","\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, \\\n","    TrainingArguments\n","from datasets import Dataset\n","import evaluate\n","\n","from masked_absa_concat_model import MaskedABSAConcatModel\n","\n","\n","def tokenize(example, tokenizer):\n","    st = list(map(lambda e: re.sub(re.escape(e[1]), '[MASK]', e[0]), zip(example['Subtext'], example['Entity'])))\n","\n","    return tokenizer(st, return_token_type_ids=True, padding='max_length', truncation=True)\n","\n","\n","def map_label(example, labels_dict):\n","    example['label'] = labels_dict[example['label']]\n","    return example\n","\n","\n","def lowercase(example):\n","    example['Subtext'] = example['Subtext'].lower()\n","    example['Entity'] = example['Entity'].lower()\n","    return example\n","\n","\n","def remove_special_characters(example):\n","    example['Subtext'] = re.sub(r'[®°™£]', '', example['Subtext'])\n","    example['Entity'] = re.sub(r'[®°™£]', '', example['Entity'])\n","    return example\n","\n","\n","def remove_whitespace_code(example):\n","    example['Subtext'] = re.sub(r'_x000D_', '', example['Subtext'])\n","\n","\n","def load_and_process_data(ds_type, label_name, tokenizer, cased=False) -> Dataset:\n","    \"\"\"\n","        Data loading and preprocessing. Does lowercase if necessary, label mapping, special character removal\n","        and tokenization\n","        @param ds_type: dataset type (train, validation, test)\n","        @param use_QA: flag whether to use QA or just target for second sequence\n","        @param label_name: original name of the label column\n","        @param tokenizer: pretrained tokenizer\n","        @param cased: whether the model was trained on cased or uncased text\n","        @return: processed HuggingFace dataset\n","        \"\"\"\n","    if ds_type == 'train':\n","        df = pd.read_csv(\"dataset/train_dataset.csv\", sep=';', encoding='utf-8')\n","        y = df[label_name]\n","        X = df.drop([label_name], axis=1)\n","        X[label_name] = y\n","        dataset = Dataset.from_pandas(X)\n","    else:\n","        dataset = Dataset.from_csv(f'dataset/{ds_type}_dataset.csv', sep=';', encoding='utf-8')\n","\n","    #if not cased:\n","        #dataset = dataset.map(lowercase)\n","    dataset = dataset.rename_column(original_column_name=label_name, new_column_name='label')\n","    labels_dict = {'negativ': 0, 'neutral': 1, 'positiv': 2, 'ambivalent': 1}   # ambivalent class removed, samples merged with neutral\n","    dataset = dataset.map(lambda e: map_label(e, labels_dict))\n","    dataset = dataset.map(remove_whitespace_code)\n","    dataset = dataset.map(remove_special_characters)\n","    dataset = dataset.filter(lambda e: e['Entity'] in e['Subtext'])\n","    dataset = dataset.map(lambda s: tokenize(s, tokenizer), batched=True)\n","\n","    return dataset\n","\n","\n","\n","\n","def experiments_automation() -> None:\n","    \"\"\"\n","        Automates the experiments so multiple experiments can be run one after the other.\n","    \"\"\"\n","    checkpoint_configs = [('bert-base-german-cased', True, 16, 4)]\n","    model_types = ['concat']\n","    experiment_name = 'target_sentiment_DE'\n","    for model_type in model_types:\n","        for checkpoint, cased, batch_size, accumulation_steps in checkpoint_configs:\n","            training_experiment(checkpoint=checkpoint,\n","                                experiment_name=experiment_name,\n","                                cased=cased,\n","                                batch_size=batch_size,\n","                                accumulation_steps=accumulation_steps,\n","                                use_custom_model=True,\n","                                model_type=model_type)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Training Experiment Function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wj_RLAeqM5aU"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","import torch\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from collections import Counter\n","from sklearn.metrics import f1_score\n","from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n","\n","def training_experiment(checkpoint, num_labels=3, experiment_name='Default', cased=False,\n","                        batch_size=16, accumulation_steps=4, use_custom_model=False, model_type='') -> None:\n","    \"\"\"\n","    The full training experiment: data loading and preprocessing, model instantiation, training and evaluation.\n","    @param checkpoint: Huggingface checkpoint for fine-tuning\n","    @param num_labels: number of labels in target\n","    @param experiment_name: name of the experiment\n","    @param cased: whether the checkpoint was trained on cased or uncased text\n","    @param batch_size: batch size for training\n","    @param accumulation_steps: number of steps for gradient accumulation\n","    @param use_custom_model: whether to use Huggingface AutoModel or custom model that uses the base from Huggingface\n","    @param model_type: type of the model (base for a regular classification model, mul and concat for TD-BERT variations)\n","    \"\"\"\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    target_sequence = 'entity'\n","    run_name = f'{model_type}-{target_sequence}-{checkpoint}'\n","\n","    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","    train = load_and_process_data('train', label_name='Sentiments', tokenizer=tokenizer, cased=cased)\n","    validation = load_and_process_data('validation', label_name='Sentiments', tokenizer=tokenizer, cased=cased)\n","    test = load_and_process_data('test', label_name='Sentiments', tokenizer=tokenizer, cased=cased)\n","    test = test.remove_columns([c for c in test.column_names if c not in\n","                                ['input_ids', 'token_type_ids', 'attention_mask', 'label']])\n","\n","    label_counts = Counter(train['label'])\n","    class_weights = [max(label_counts.values()) / label_counts[cls] for cls in sorted(set(train['label']))]\n","\n","    custom_models = {'concat': MaskedABSAConcatModel}\n","    if use_custom_model and model_type in custom_models:\n","        cls_id, mask_id, sep_id = tokenizer.encode('[MASK]')\n","        model = custom_models[model_type](checkpoint, num_labels=num_labels, class_weights=class_weights,\n","                                          cls_id=cls_id, sep_id=sep_id, mask_id=mask_id)\n","    else:\n","        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n","\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","    model_dir = osp.join('models', 'custom', model_type + '1', '_'.join([experiment_name, run_name]))\n","    print(f'model_dir: {model_dir}')\n","    training_args = TrainingArguments(\n","        model_dir,\n","        per_device_train_batch_size=batch_size,\n","        num_train_epochs=1,\n","        evaluation_strategy='steps',\n","        logging_steps=200,\n","        save_steps=200,\n","        save_total_limit=10,\n","        load_best_model_at_end=True,\n","        gradient_accumulation_steps=accumulation_steps,\n","        metric_for_best_model='eval_loss',\n","        resume_from_checkpoint=os.path.join('models/custom/concat1/target_sentiment_DE_concat-entity-bert-base-german-cased/', 'checkpoint-5600')\n","    )\n","    trainer = Trainer(\n","        model,\n","        training_args,\n","        train_dataset=train,\n","        eval_dataset=validation,\n","        data_collator=data_collator,\n","        tokenizer=tokenizer\n","    )\n","    try:\n","        trainer.train(resume_from_checkpoint=True)\n","    except ValueError:\n","        trainer.train()\n","\n","    trainer.save_model(\"models/custom/concat1/target_sentiment_DE_concat-entity-bert-base-german-cased/\")\n","    #torch.save(model, \"models/bestmodel.pth\")\n","    torch.save(model.state_dict(), \"models/bestmodel_state_dict.pth\")\n","    \n","    # Evaluation\n","\n","    model.eval()\n","    test.set_format('torch')\n","    # Load metrics for evaluation\n","    metrics = {\n","        'precision': evaluate.load('precision'),\n","        'recall': evaluate.load('recall'),\n","        'f1': evaluate.load('f1')\n","    }\n","    train_predictions = trainer.predict(train).predictions.argmax(axis=1)\n","    train_labels = train['label']\n","    train_f1 = f1_score(train_labels, train_predictions, average='weighted')\n","\n","    # Calculate F1 score for testing data\n","    test_predictions = trainer.predict(test).predictions.argmax(axis=1)\n","    test_labels = test['label']\n","    test_f1 = f1_score(test_labels, test_predictions, average='weighted')\n","\n","    # Print or log the F1 scores\n","    print(f'Training F1 Score: {train_f1}')\n","    print(f'Testing F1 Score: {test_f1}')\n","\n","\n","    dataloader = DataLoader(test, batch_size=64)\n","    for batch in tqdm(dataloader):\n","        with torch.no_grad():\n","            outputs = model(**{k: v.to(device) for k, v in batch.items()\n","                               if k in ['input_ids', 'token_type_ids', 'attention_mask']}).logits.argmax(dim=1)\n","        for v in metrics.values():\n","            v.add_batch(predictions=outputs,\n","                        references=batch['label'])\n","    labels = ['negative', 'neutral', 'positive']\n","    # Print metrics (replace with your preferred logging or result handling)\n","    for k, v in metrics.items():\n","        metric_values = v.compute(average=None)\n","        for i, mv in enumerate(metric_values[k]):\n","            print(f'{k}.{labels[i]}: {mv}')\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Running the Training Experiment:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaDrXgzGzaim"},"outputs":[],"source":["#checkpoint_path = 'bert-base-german-cased'\n","checkpoint_path='bert-base-german-cased'\n","# Run the training experiment\n","training_experiment(checkpoint=checkpoint_path ,\n","                    experiment_name='target_sentiment_DE_long',\n","                    cased=True,\n","                    batch_size=16,\n","                    accumulation_steps=4,\n","                    use_custom_model=True,\n","                    model_type='concat')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOOlEEt38OCt2BRX2nkrY5e","gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
