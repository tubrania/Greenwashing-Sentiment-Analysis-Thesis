{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2560,"status":"ok","timestamp":1700578562724,"user":{"displayName":"Rania Mokni","userId":"17280758072772539145"},"user_tz":-60},"id":"q-fb1W2mZ2zA","outputId":"ddfae3d2-6196-4f39-a793-bff60c7aaf1c"},"outputs":[],"source":["# Mount the Google Drve for getting dataset\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change directory to the project directory\n","\n","import os\n","os.chdir('/content/drive/MyDrive/TD-bert/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5777,"status":"ok","timestamp":1700578570495,"user":{"displayName":"Rania Mokni","userId":"17280758072772539145"},"user_tz":-60},"id":"P40rbquVaHUf","outputId":"6879305a-6e8a-4dfe-cf48-029f456be612"},"outputs":[],"source":["#Install the Hugging Face Transformers library:\n","\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4TGxtWJaIMC"},"outputs":[],"source":["# Import the necessary modules:\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t67D8qgeaLbK"},"outputs":[],"source":["# Loading the pretrained BERT tokenizer:\n","\n","tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5977,"status":"ok","timestamp":1700578592062,"user":{"displayName":"Rania Mokni","userId":"17280758072772539145"},"user_tz":-60},"id":"q1NZq77CZJDa","outputId":"9359e1cd-bfb3-4ccd-95a3-6fb1081010ce"},"outputs":[],"source":["# Load the pre-trained  BERT model and prepare it for sequence classification:\n","\n","num_labels = 3  # positive, negative, and neutral\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=num_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3372,"status":"ok","timestamp":1700578595426,"user":{"displayName":"Rania Mokni","userId":"17280758072772539145"},"user_tz":-60},"id":"hjXiw03gXqYb","outputId":"d16779e6-df0a-4cd8-bd8d-25864bc73fde"},"outputs":[],"source":["# Adding the [TAR] token to the tokenizer's vocabulary and resizing the model's embeddings:\n","\n","tokenizer.add_tokens([\"[TAR]\"])\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJhwHS5AaVN8"},"outputs":[],"source":["# Defining a custom dataset class for sentient analysis:\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=128):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","\n","        encoding = self.tokenizer(text, add_special_tokens=False, max_length=self.max_length, truncation=True, padding='max_length')\n","        input_ids = encoding[\"input_ids\"]\n","        attention_mask = encoding[\"attention_mask\"]\n","\n","        return {\n","            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n","            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n","            \"label\": torch.tensor(label, dtype=torch.long)\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":1654,"status":"ok","timestamp":1700578610867,"user":{"displayName":"Rania Mokni","userId":"17280758072772539145"},"user_tz":-60},"id":"zDMTkAAEalIY","outputId":"d3a951ea-25d4-4047-8019-b20562a9d6c2"},"outputs":[],"source":["# Create the train, validation, and test sets:\n","train_data = pd.read_csv(\"dataset/tar_train_dataset.csv\", sep=';', encoding='utf-8')\n","validation_data = pd.read_csv(\"dataset/tar_validation_dataset.csv\", sep=';', encoding='utf-8')\n","test_data = pd.read_csv(\"dataset/tar_test_dataset.csv\", sep=';', encoding='utf-8')\n","\n","train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9r5OiWDiauAS"},"outputs":[],"source":["# For the T-BERT model, training, validation and test will be done by\n","# using targeted sentiments:\n","\n","train_texts = train_data[\"Text\"]\n","train_labels = train_data[\"Targeted Sentiment\"]\n","val_texts = validation_data[\"Text\"]\n","val_labels = validation_data[\"Targeted Sentiment\"]\n","\n","test_texts = test_data[\"Text\"]\n","test_labels = test_data[\"Targeted Sentiment\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLxrE3JCbkts"},"outputs":[],"source":["# Convert string labels to numerical values\n","label_mapping = {\"negativ\": 0, \"neutral\": 1, \"positiv\": 2}\n","train_labels = [label_mapping[label] for label in train_labels]\n","val_labels = [label_mapping[label] for label in val_labels]\n","test_labels = [label_mapping[label] for label in test_labels]\n","\n","# Create SentimentDataset instances\n","train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n","val_dataset = SentimentDataset(val_texts, val_labels, tokenizer)\n","test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)\n","\n","# Create DataLoader instances\n","batch_size = 24\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":330,"status":"ok","timestamp":1700578621426,"user":{"displayName":"Rania Mokni","userId":"17280758072772539145"},"user_tz":-60},"id":"bqi_Ky4WciV5","outputId":"105abca2-2e25-47e6-cbe2-f6f85e1556e6"},"outputs":[],"source":["# Set the device and move the model to the device:\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)\n","\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":383,"status":"ok","timestamp":1700578630930,"user":{"displayName":"Rania Mokni","userId":"17280758072772539145"},"user_tz":-60},"id":"Z3PFc8GTcCiY","outputId":"15577203-afd3-4adf-e4fa-6849771299d6"},"outputs":[],"source":["# Get class counts for training dataset:\n","\n","class_count_0 = train_labels.count(0)\n","class_count_1 = train_labels.count(1)\n","class_count_2 = train_labels.count(2)\n","\n","def getKey(dict, value):\n","  return list(filter(lambda x: dict[x] == value, dict))[0]\n","\n","print(\"Class counts:\")\n","print(f\"{getKey(label_mapping,0)}: {class_count_0}\")\n","print(f\"{getKey(label_mapping,1)}: {class_count_1}\")\n","print(f\"{getKey(label_mapping,2)}: {class_count_2}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgeJI-Mabzd9"},"outputs":[],"source":["# Set the class weights and define the loss function:\n","\n","from torch import nn\n","import torch.nn.functional as F\n","\n","class_weights = torch.tensor([1 / class_count_0, 1 / class_count_1, 1 / class_count_2]).to(device)\n","loss_fn = nn.CrossEntropyLoss(weight=class_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1700578637028,"user":{"displayName":"Rania Mokni","userId":"17280758072772539145"},"user_tz":-60},"id":"vcv3hIvnb3CT","outputId":"31c78bfe-8369-451c-8918-c13f18f8aab2"},"outputs":[],"source":["# Prepare the optimizer and learning rate scheduler:\n","\n","epochs = 2\n","num_training_steps = epochs * len(train_loader)\n","lr = 1e-5\n","weight_decay = 0.1\n","warmup_steps = 300\n","\n","optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXDfdPfac2Px"},"outputs":[],"source":["# Define helper functions for training and evaluation:\n","\n","def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n","    model = model.train()\n","    total_train_loss = 0\n","\n","    for batch in data_loader:\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"label\"].to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","\n","        total_train_loss += loss.item()\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","    return total_train_loss / len(data_loader)\n","\n","def eval_epoch(model, data_loader, loss_fn, device):\n","    model = model.eval()\n","    total_eval_loss = 0\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"label\"].to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","\n","            total_eval_loss += loss.item()\n","\n","    return total_eval_loss / len(data_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1Y-uYNAPVia"},"outputs":[],"source":["# Calculate performance metrics:\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","def get_predictions(model, data_loader, device):\n","    model = model.eval()\n","    predictions = []\n","    true_labels = []\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"label\"].to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            _, preds = torch.max(outputs.logits, dim=1)\n","\n","            predictions.extend(preds.cpu().numpy().tolist())\n","            true_labels.extend(labels.cpu().numpy().tolist())\n","\n","    return predictions, true_labels\n","\n","def get_f1_scores(model, device):\n","    print(\"F1 Scores:\")\n","    train_preds, train_labels= get_predictions(model, train_loader, device)\n","    train_f1 = precision_recall_fscore_support(train_preds, train_labels, average='weighted', zero_division=0)[2]\n","    print(f\"\\tTrain dataset against targeted sentiment labels: {train_f1:.4f}\")\n","\n","    test_preds, test_labels = get_predictions(model, test_loader, device)\n","    test_f1 = precision_recall_fscore_support(test_labels, test_preds, average='weighted', zero_division=0)[2]\n","    print(f\"\\tTest dataset against targeted sentiment labels: {test_f1:.4f}\")\n","\n","    return train_f1, test_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3900649,"status":"ok","timestamp":1700582558280,"user":{"displayName":"Rania Mokni","userId":"17280758072772539145"},"user_tz":-60},"id":"NsDDjfeWdAGe","outputId":"9910a164-3ccc-4fbd-8d82-98aefd02bfc7"},"outputs":[],"source":["import csv\n","\n","f = open(\"./models/t-bert_marked/results.csv\",\"w\")\n","writer = csv.writer(f)\n","writer.writerow([\"Epoch\",\"Train loss\", \"Val. loss\", \"Test loss\",\"Train data F1\", \"Test data F1\"])\n","\n","# Train and evaluate the model:\n","\n","for epoch in range(epochs):\n","    print(\"-\" * 10)\n","    print(f\"Epoch {epoch+1}/{epochs}\")\n","\n","    train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device, scheduler)\n","    print(f\"Train Loss: {train_loss:.4f}\")\n","\n","    val_loss = eval_epoch(model, val_loader, loss_fn, device)\n","    print(f\"Validation Loss: {val_loss:.4f}\")\n","\n","    test_loss = \"-\"\n","\n","    test_loss = eval_epoch(model, test_loader, loss_fn, device)\n","    print(f\"Test Loss: {test_loss:.4f}\")\n","\n","    model.save_pretrained(f\"./models/t-bert_marked/epoch{epoch+1}/model\")\n","    tokenizer.save_pretrained(f\"./models/t-bert_marked/epoch{epoch+1}/tokenizer\")\n","    print(\"Saved.\")\n","\n","    train_f1, test_f1 = get_f1_scores(model, device)\n","\n","    writer.writerow([epoch+1,train_loss,val_loss,test_loss,train_f1,test_f1])\n","\n","f.close()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
