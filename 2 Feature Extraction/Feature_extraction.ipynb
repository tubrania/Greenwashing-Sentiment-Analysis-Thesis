{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15bd949",
   "metadata": {},
   "source": [
    "## Feature extraction using python code\n",
    "In the end, we have selected only the following features from our code: <br>\n",
    "(future tense, past tense, sentence count, type-token ratio, content-word diversity, and the total number of first-person pronouns)<br>\n",
    "The rest are derived from features extracted using LIWC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11300bc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f0a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0599829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import regexp_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e5882f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e50b0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "nlp_en.max_length = 1700000 \n",
    "nlp_de.max_length = 1700000 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db73211",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0dd31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to the CSV file in the \"data\" directory\n",
    "file_path = \"../data/reports/sentences_from_reports.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the 'sentence' column is of string data type\n",
    "df['sentence'] = df['sentence'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b115d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def espace(text) : \n",
    "    text = text+' '\n",
    "    return text\n",
    "df[\"sentence\"] = df[\"sentence\"].apply(espace)\n",
    "sentences = df.groupby(['company','year'],as_index=False)['sentence'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c5809c",
   "metadata": {},
   "source": [
    "### Utill Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258ff205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "        text = str(text)  # convert into string\n",
    "        text = text.lower()  # convert to lower case\n",
    "        text = re.sub('[0-9]', '', text)  # remove numbers\n",
    "        text = re.sub(r'\\[.+?\\-,_]', ' ', text)  # remove bracket\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc15be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence'] = df['sentence'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01da1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detect the language of the given text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    str: Detected language ('en' for English, 'de' for German), or 'unknown' if language detection failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        detected_lang = detect(text)\n",
    "        if detected_lang == 'en':\n",
    "            return 'en'\n",
    "        elif detected_lang == 'de':\n",
    "            return 'de'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    except Exception as e:\n",
    "        print(\"Language detection failed.\")\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef598d",
   "metadata": {},
   "source": [
    "### Linguistic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523e0af",
   "metadata": {},
   "source": [
    "#### Feature: word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c0bb4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(raw, min_length = 1):\n",
    "    \"\"\" Function to count the number of words in a passage of text.\n",
    "        Supplying parameter 'min_length' gives number of words with\n",
    "        at least min_length letters.\n",
    "    \"\"\"\n",
    "    words = re.sub(r'[^\\w\\s]',' ',raw)\n",
    "    text = re.sub('[0-9]', '', words)\n",
    "    tokens = nltk.word_tokenize(words)\n",
    "    return len([word for word in tokens if len(word) >= min_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f00db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_counts'] = df['sentence'].apply(word_count)\n",
    "df_word_counts = df.groupby(['company','year'],as_index=False)['word_counts'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732594f",
   "metadata": {},
   "source": [
    "#### Feature: sentence count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f592cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sen_counts = df.groupby(['company','year'],as_index=False).size()\n",
    "df_sen_counts = df_sen_counts.rename(columns={\"size\": \"sentence_counts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6d255e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([df_word_counts , df_sen_counts[\"sentence_counts\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e33f0",
   "metadata": {},
   "source": [
    "#### Feature: type-token ratio\n",
    "The number of unique words was divided by the total number of words within each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "849ce982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTypeData(tokenList):\n",
    "        \"\"\"\n",
    "    Calculate type-based data for a given token list.\n",
    "\n",
    "    Parameters:\n",
    "    tokenList (list): List of tokens.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame containing type-based data (type, frequency, probability, rank, cumulative probability, part of speech).\n",
    "    \"\"\"\n",
    "        typeData = pd.DataFrame()\n",
    "        freqDist = FreqDist(tokenList)\n",
    "        typeNum = len(freqDist)\n",
    "        typeData['type'] = FreqDist(tokenList).keys()\n",
    "        typeData['freq'] = [freqDist[i] for i in typeData['type']]\n",
    "        tokenNum = np.sum(typeData['freq'])\n",
    "        typeData = typeData.sort_values('freq', ascending=False)\n",
    "        typeData.index = range(typeNum)\n",
    "        typeData['prob'] = [freq / tokenNum for freq in typeData['freq']]\n",
    "        typeData['rank'] = list(range(1, typeNum + 1))\n",
    "        typeData['cumProb'] = [np.sum(typeData['prob'][:i]) for i in\n",
    "                               range(1, typeNum + 1)]\n",
    "        typeData['pos'] = [i[1] for i in pos_tag(typeData['type'])]\n",
    "        return typeData\n",
    "\n",
    "def getTTR(rawtext):\n",
    "        \"\"\"\n",
    "    Calculate type-token ratio (TTR) for a given text.\n",
    "\n",
    "    Parameters:\n",
    "    rawtext (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Type-token ratio (TTR) for the input text.\n",
    "    \"\"\"\n",
    "        text = cleanText(rawtext)  # clean text\n",
    "        tokenList = regexp_tokenize(text, '\\w+')  # tokenize\n",
    "        tokenNum = len(tokenList)  # calculate token number\n",
    "        typeData = getTypeData(tokenList)\n",
    "        typeNum = len(typeData)\n",
    "        ttr = typeNum / tokenNum\n",
    "        return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e4e6cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"type_token\"] = sentences[\"sentence\"].apply(getTTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011244c",
   "metadata": {},
   "source": [
    "#### feature: content-word diversity\n",
    "Content (or lexical) words according to consist of nouns, main verbs, adjectives and adverbs. <br>\n",
    "The number of content words was divided by the total number of words within each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f9678ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_content_word_diversity(text):\n",
    "    \"\"\"\n",
    "    Calculate Content Word Diversity for a given text.\n",
    "    Content words consist of nouns, main verbs, adjectives, and adverbs.\n",
    "    Content Word Diversity = (Number of content words) / (Total number of words)\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text for which Content Word Diversity is to be calculated.\n",
    "\n",
    "    Returns:\n",
    "    float: Content Word Diversity for the input text.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text.lower())\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    content_word_count = sum(1 for word, pos in pos_tags if pos.startswith(('N', 'V', 'R', 'J')))\n",
    "    total_word_count = len(words)\n",
    "    content_word_diversity = content_word_count / total_word_count\n",
    "    return content_word_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21b8ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"content_word_diversity\"] = sentences[\"sentence\"].apply(calculate_content_word_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3eb6ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed5953",
   "metadata": {},
   "source": [
    "#### features : average sentence length and average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f7f66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_length(text):\n",
    "    \"\"\"\n",
    "    Calculate the average sentence length in terms of words in a given text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Average sentence length (in words).\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    total_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
    "    total_sentences = len(sentences)\n",
    "    if total_sentences == 0:\n",
    "        return 0\n",
    "    average_length = total_words / total_sentences\n",
    "    return average_length\n",
    "\n",
    "def average_word_length(text):\n",
    "    \"\"\"\n",
    "    Calculate the average word length in characters in a given text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Average word length (in characters).\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    total_words = len(words)\n",
    "    if total_words == 0:\n",
    "        return 0\n",
    "    average_length = total_characters / total_words\n",
    "    return average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d6f2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"average_sentence_length\"] = sentences[\"sentence\"].apply(average_sentence_length)\n",
    "features[\"average_word_length\"] = sentences[\"sentence\"].apply(average_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "195eb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b29d47",
   "metadata": {},
   "source": [
    "#### features : Space Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7e76706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posTag(rawText):\n",
    "    \"\"\"\n",
    "    This function performs part-of-speech tagging on the input raw text using NLTK.\n",
    "    \n",
    "    Parameters:\n",
    "    rawText (str): The raw text to be tagged for part-of-speech.\n",
    "    \n",
    "    Returns:\n",
    "    str: A string representing the tagged text, where each word is combined with its\n",
    "         corresponding part-of-speech tag in the format 'word_POS'.\n",
    "    \"\"\"\n",
    "    # Perform part-of-speech tagging on the raw text\n",
    "    tagList = pos_tag(word_tokenize(rawText))\n",
    "    \n",
    "    # Construct a list of words and their part-of-speech tags in a specific format (word_POS)\n",
    "    resultList = [i[0].lower() + '_' + i[1] for i in tagList]\n",
    "    \n",
    "    # Join the words and their part-of-speech tags to form the tagged text\n",
    "    taggedText = ' '.join(resultList)\n",
    "    \n",
    "    # Return the tagged text\n",
    "    return taggedText\n",
    "\n",
    "    \n",
    "def calculate_spatial_adverbial_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of spatial adverbials for the given language using part-of-speech tagging.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Ratio of spatial adverbials.\n",
    "    \"\"\"\n",
    "    # Clean the text\n",
    "    text = cleanText(text)\n",
    "    language = detect_language(text)\n",
    "    \n",
    "    # Tokenize the text using regexp_tokenize\n",
    "    tokenList = regexp_tokenize(text, '\\w+')\n",
    "    tokenNum = len(tokenList)\n",
    "    \n",
    "    # Perform part-of-speech tagging and join into a tagged text\n",
    "    taggedText = posTag(text)\n",
    "\n",
    "    # Define the pattern to match spatial adverbials based on the provided list\n",
    "    english_spatial_pattern = r'( (aboard|above|abroad|across|ahead|alongside|around' \\\n",
    "                              r'|ashore|astern|away|behind|below|beneath|beside|downhill' \\\n",
    "                              r'|downstairs|downstream|east|far|here|hereabouts|indoors|inland' \\\n",
    "                              r'|inshore|inside|locally|near|nearby|north|nowhere|outdoors' \\\n",
    "                              r'|outside|overboard|overland|overseas|south|underfoot|there' \\\n",
    "                              r'|underground|underneath|uphill|upstairs|upstream|west)' \\\n",
    "                              r'_[A-Z]+)'\n",
    "\n",
    "    german_spatial_pattern = r'( (hier| dort|dorthin|oben|unten|nahe|weit|innen|außen'\\\n",
    "                                r'|an Bord|überall|quer|voraus|entlang|um|an Land|achtern'\\\n",
    "                                r'|weg|hinter|neben|bergab|unten|stromabwärts|östlich|hier'\\\n",
    "                                r'|in der Nähe|drinnen|im Landesinneren|an Land|lokal|in der Nähe'\\\n",
    "                                r'|nördlich|nirgendwo|im Freien|über Bord|über Land|überseeisch'\\\n",
    "                                r'|südlich|unter den Füßen|unterirdisch|unterhalb|bergauf|oben|stromaufwärts|westlich)'\\\n",
    "                                r'_[A-Z]+)'\n",
    "    \n",
    "    # Select the appropriate pattern based on the language\n",
    "    if language == 'en':\n",
    "        pattern = english_spatial_pattern\n",
    "    elif language == 'de':\n",
    "        pattern = german_spatial_pattern\n",
    "    else:\n",
    "        pattern = german_spatial_pattern\n",
    "        #print (text)\n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Count the number of matches for the pattern (spatial adverbials)\n",
    "    num = len(re.findall(pattern, taggedText))\n",
    "\n",
    "    # Calculate the ratio of spatial adverbials\n",
    "    spatial_adverbial_ratio = 100 * num / tokenNum if tokenNum != 0 else 0\n",
    "    return spatial_adverbial_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ddb305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"spatial_adverbial_ratio\"] = sentences[\"sentence\"].apply(calculate_spatial_adverbial_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726e5514",
   "metadata": {},
   "source": [
    "#### features : Time Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a454c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time_adverbial_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of time adverbials for English and German using part-of-speech tagging.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Ratio of time adverbials.\n",
    "    \"\"\"\n",
    "    # Clean the text and detect the language\n",
    "    cleaned_text = cleanText(text)\n",
    "    language = detect_language(text)\n",
    "    \n",
    "    # Tokenize the text using regexp_tokenize\n",
    "    tokenList = regexp_tokenize(cleaned_text, '\\w+')\n",
    "    tokenNum = len(tokenList)\n",
    "\n",
    "    # Perform part-of-speech tagging and join into a tagged text\n",
    "    taggedText = posTag(text)\n",
    "\n",
    "    # Define the patterns for time adverbials\n",
    "    english_time_pattern =  r'( (afterwards|again|earlier|early|eventually|formerly' \\\n",
    "                            r'|immediately|initially|instantly|late|lately|later' \\\n",
    "                            r'|momentarily|now|nowadays|once|originally|presently' \\\n",
    "                            r'|previously|recently|shortly|simultaneously|soon' \\\n",
    "                            r'|subsequently|today|tomorrow|tonight|yesterday' \\\n",
    "                            r'|morning|afternoon|evening|night)' \\\n",
    "                            r'_[A-Z]+)'\n",
    "    \n",
    "    german_time_pattern =  r'( (nachher|wieder|früher|früh|schließlich|ehemals'\\\n",
    "                           r'|sofort|anfänglich|augenblicklich|spät|Zeit|später'\\\n",
    "                           r'|moment|jetzt|heutzutage|einmal|ursprünglich|gegenwärtig'\\\n",
    "                           r'|früher|kürzlich|bald|gleichzeitig|bald|anschließend'\\\n",
    "                           r'|morgen|nachmittag|abend|nachts)'\\\n",
    "                           r'_[A-Z]+)'\n",
    "\n",
    "    # Select the appropriate pattern based on the language\n",
    "    if language == 'en':\n",
    "        pattern = english_time_pattern\n",
    "    elif language == 'de':\n",
    "        pattern = german_time_pattern\n",
    "    else:\n",
    "        pattern = german_time_pattern\n",
    "        \n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Count the number of matches for the pattern (time adverbials)\n",
    "    num = len(re.findall(pattern, taggedText))\n",
    "    \n",
    "    # Calculate the ratio of time adverbials\n",
    "    time_adverbial_ratio = 100 * num / tokenNum if tokenNum != 0 else 0\n",
    "    return time_adverbial_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84e81f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"time_adverbial_ratio\"] = sentences[\"sentence\"].apply(calculate_time_adverbial_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6da32d",
   "metadata": {},
   "source": [
    "#### features : Generalizing word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca0af9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_generalizing_term_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of generalizing terms to the total number of words in the text for English and German.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Ratio of generalizing terms to the total number of words.\n",
    "    \"\"\"\n",
    "    # Detect the language of the text\n",
    "    cleaned_text = cleanText(text)\n",
    "    language = detect_language(text)\n",
    "\n",
    "    if language not in ['en', 'de']:\n",
    "        raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Process the text with the appropriate spaCy model\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(cleaned_text, disable=['ner', 'parser'])\n",
    "        generalizing_terms = {'often', 'sometimes', 'usually', 'generally', 'typically', 'commonly', 'mostly', 'general'}\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(cleaned_text, disable=['ner', 'parser'])\n",
    "        generalizing_terms = {'oft', 'manchmal', 'normalerweise', 'generell', 'typischerweise', 'üblicherweise', 'meistens', 'allgemein'}\n",
    "    else:\n",
    "        doc = nlp_de(cleaned_text, disable=['ner', 'parser'])\n",
    "        generalizing_terms = {'oft', 'manchmal', 'normalerweise', 'generell', 'typischerweise', 'üblicherweise', 'meistens', 'allgemein'}\n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Count the number of generalizing terms\n",
    "    \n",
    "    generalizing_term_count = sum(1 for token in doc if token.text.lower() in generalizing_terms)\n",
    "\n",
    "    # Total number of words\n",
    "    total_words = len(doc)\n",
    "\n",
    "    # Calculate the ratio\n",
    "    ratio = (generalizing_term_count / total_words) * 100 if total_words != 0 else 0\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c349b42",
   "metadata": {},
   "source": [
    "#### features : Count modal verbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19ec3afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_modal_verb_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of modal verbs to the total number of words in the text using spaCy.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Ratio of modal verbs to the total number of words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean the text and detect the language\n",
    "    cleaned_text = cleanText(text)\n",
    "    language = detect_language(text)\n",
    "\n",
    "    # Process the text with the appropriate spaCy model\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(cleaned_text, disable=['ner', 'parser'])\n",
    "        modal_verbs = {'can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must'}\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(cleaned_text, disable=['ner', 'parser'])\n",
    "        modal_verbs = {'können', 'kann', 'könnte', 'will', 'würde', 'soll', 'sollte', 'mögen', 'müsste', 'muss'}\n",
    "    else:\n",
    "        doc = nlp_de(cleaned_text, disable=['ner', 'parser'])\n",
    "        modal_verbs = {'können', 'kann', 'könnte', 'will', 'würde', 'soll', 'sollte', 'mögen', 'müsste', 'muss'}\n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Count the number of modal verbs\n",
    "    modal_verb_count = sum(1 for token in doc if token.text.lower() in modal_verbs)\n",
    "\n",
    "    # Total number of words\n",
    "    total_words = len(doc)\n",
    "\n",
    "    # Calculate the ratio\n",
    "    ratio = (modal_verb_count / total_words) * 100 if total_words != 0 else 0\n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdb4d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"modal_verb_ratio\"] = sentences[\"sentence\"].apply(calculate_modal_verb_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8034127c",
   "metadata": {},
   "source": [
    "#### features : Total first person / Impersonal  pronouns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6f77d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_first_person_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of total first-person pronouns to the total number of words in the text using spaCy.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Ratio of total first-person pronouns to the total number of words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean the text and detect the language\n",
    "    cleaned_text = cleanText(text)\n",
    "    language = detect_language(text)\n",
    "\n",
    "    # Process the text with the appropriate spaCy model\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(cleaned_text , disable=['ner', 'parser'])\n",
    "        first_person_pronouns = {'I', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'}\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(cleaned_text , disable=['ner', 'parser'])\n",
    "        first_person_pronouns = {'ich', 'mich', 'mein', 'meine', 'mir', 'wir', 'uns', 'unser', 'unsere', 'uns'}\n",
    "    else:\n",
    "        doc = nlp_de(cleaned_text , disable=['ner', 'parser'])\n",
    "        first_person_pronouns = {'ich', 'mich', 'mein', 'meine', 'mir', 'wir', 'uns', 'unser', 'unsere', 'uns'}\n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Count the number of first-person pronouns\n",
    "    first_person_count = sum(1 for token in doc if token.text.lower() in first_person_pronouns)\n",
    "\n",
    "    # Total number of words\n",
    "    total_words = len(doc)\n",
    "\n",
    "    # Calculate the ratio\n",
    "    ratio = (first_person_count / total_words) * 100 if total_words != 0 else 0\n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b83461ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_impersonal_pronoun_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of impersonal pronouns to the total number of words in the text using spaCy.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Ratio of impersonal pronouns to the total number of words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean the text and detect the language\n",
    "    cleaned_text = cleanText(text)\n",
    "    language = detect_language(text)\n",
    "\n",
    "    # Process the text with the appropriate spaCy model\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(cleaned_text , disable=['ner', 'parser'])\n",
    "        impersonal_pronouns = {'it', 'its'}\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(cleaned_text , disable=['ner', 'parser'])\n",
    "        impersonal_pronouns = {'es'}\n",
    "    else:\n",
    "        impersonal_pronouns = {'es'}\n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Count the number of impersonal pronouns\n",
    "    impersonal_pronoun_count = sum(1 for token in doc if token.text.lower() in impersonal_pronouns)\n",
    "\n",
    "    # Total number of words\n",
    "    total_words = len(doc)\n",
    "\n",
    "    # Calculate the ratio\n",
    "    ratio = (impersonal_pronoun_count / total_words) * 100 if total_words != 0 else 0\n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c299b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"first_person_ratio\"] = sentences[\"sentence\"].apply(calculate_first_person_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc330e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"impersonal_pronoun_ratio\"] = sentences[\"sentence\"].apply(calculate_impersonal_pronoun_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab2211",
   "metadata": {},
   "source": [
    "#### features : Negations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a841fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_negation_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of negation words to the total number of words in the text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Ratio of negation words to the total number of words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean the text and detect the language\n",
    "    cleaned_text = cleanText(text)\n",
    "    language = detect_language(text)\n",
    "\n",
    "    # Process the text with the appropriate spaCy model\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(cleaned_text, disable=['ner', 'parser'])\n",
    "        negation_words = {'not', \"n't\", 'never', 'no', 'nothing', 'neither', 'nowhere', 'none', 'nor'}\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(cleaned_text, disable=['ner', 'parser'])\n",
    "        negation_words = {'nicht', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'nirgendwo', 'niemals', 'nie'}\n",
    "    else:\n",
    "        doc = nlp_de(cleaned_text, disable=['ner', 'parser'])\n",
    "        negation_words = {'nicht', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'nirgendwo', 'niemals', 'nie'}\n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Count the number of negation words\n",
    "    negation_count = sum(1 for token in doc if token.text.lower() in negation_words)\n",
    "\n",
    "    # Total number of words\n",
    "    total_words = len(doc)\n",
    "\n",
    "    # Calculate the ratio\n",
    "    ratio = (negation_count / total_words) * 100 if total_words != 0 else 0\n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a64b8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"negation_ratio\"] = sentences[\"sentence\"].apply(calculate_negation_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6efd6e",
   "metadata": {},
   "source": [
    "#### features : Conjuctions Exclusive/ Inclusive\n",
    "maybe i will use LIWC to calculate exclusive/inclusive because i can't finde the dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "320fd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exclusive_conjunction_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of exclusive conjunctions to the total number of words in the text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Ratio of exclusive conjunctions to the total number of words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean the text and detect the language\n",
    "    cleaned_text = cleanText(text)\n",
    "    language = detect_language(text)\n",
    "\n",
    "    # Process the text with the appropriate spaCy model\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(cleaned_text, disable=['ner', 'parser'])\n",
    "        exclusive_conjunctions = {'but', 'yet'}\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(cleaned_text, disable=['ner', 'parser'])\n",
    "        exclusive_conjunctions = {'aber', 'sondern'}\n",
    "    else:\n",
    "        doc = nlp_de(cleaned_text, disable=['ner', 'parser'])\n",
    "        exclusive_conjunctions = {'aber', 'sondern'}\n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Count the number of exclusive conjunctions\n",
    "    exclusive_conjunction_count = sum(1 for token in doc if token.text.lower() in exclusive_conjunctions)\n",
    "\n",
    "    # Total number of words\n",
    "    total_words = len(doc)\n",
    "\n",
    "    # Calculate the ratio\n",
    "    ratio = (exclusive_conjunction_count / total_words) * 100 if total_words != 0 else 0\n",
    "    return ratio\n",
    "\n",
    "\n",
    "def calculate_inclusive_conjunction_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of inclusive conjunctions to the total number of words in the text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Ratio of inclusive conjunctions to the total number of words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean the text and detect the language\n",
    "    cleaned_text = cleanText(text)\n",
    "    language = detect_language(text)\n",
    "\n",
    "    # Process the text with the appropriate spaCy model\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(cleaned_text, disable=['ner', 'parser'])\n",
    "        inclusive_conjunctions = {'and', 'or'}\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(cleaned_text, disable=['ner', 'parser'])\n",
    "        inclusive_conjunctions = {'und', 'oder'}\n",
    "    else:\n",
    "        doc = nlp_de(cleaned_text, disable=['ner', 'parser'])\n",
    "        inclusive_conjunctions = {'und', 'oder'}\n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Count the number of inclusive conjunctions\n",
    "    inclusive_conjunction_count = sum(1 for token in doc if token.text.lower() in inclusive_conjunctions)\n",
    "\n",
    "    # Total number of words\n",
    "    total_words = len(doc)\n",
    "\n",
    "    # Calculate the ratio\n",
    "    ratio = (inclusive_conjunction_count / total_words) * 100 if total_words != 0 else 0\n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13c82ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"exclusive_conjunction_ratio\"] = sentences[\"sentence\"].apply(calculate_exclusive_conjunction_ratio)\n",
    "features[\"inclusive_conjunction_ratio\"] = sentences[\"sentence\"].apply(calculate_inclusive_conjunction_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b889605",
   "metadata": {},
   "source": [
    "#### features : Tentative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77eec808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tentative_word_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of tentative words to the total number of words in the text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Ratio of tentative words to the total number of words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean the text and detect the language\n",
    "    cleaned_text = cleanText(text)\n",
    "    language = detect_language(text)\n",
    "\n",
    "    # Process the text with the appropriate spaCy model\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(cleaned_text, disable=['ner', 'parser'])\n",
    "        tentative_words = {'maybe', 'perhaps', 'possibly', 'probably', 'uncertain', 'doubt', 'likely', 'would', 'could'}\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(cleaned_text, disable=['ner', 'parser'])\n",
    "        tentative_words = {'vielleicht', 'möglicherweise', 'eventuell', 'wahrscheinlich', 'unsicher', 'zweifel', 'vermutlich', 'würde', 'könnte'}\n",
    "    else:\n",
    "        doc = nlp_de(cleaned_text, disable=['ner', 'parser'])\n",
    "        tentative_words = {'vielleicht', 'möglicherweise', 'eventuell', 'wahrscheinlich', 'unsicher', 'zweifel', 'vermutlich', 'würde', 'könnte'}\n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Count the number of tentative words\n",
    "    tentative_word_count = sum(1 for token in doc if token.text.lower() in tentative_words)\n",
    "\n",
    "    # Total number of words\n",
    "    total_words = len(doc)\n",
    "\n",
    "    # Calculate the ratio\n",
    "    ratio = (tentative_word_count / total_words) * 100 if total_words != 0 else 0\n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed29fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"tentative_word_ratio\"] = sentences[\"sentence\"].apply(calculate_tentative_word_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9091b6",
   "metadata": {},
   "source": [
    "#### features : Informal\n",
    "maybe i will use LIWC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c40f6",
   "metadata": {},
   "source": [
    "#### features : Count Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5025fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_count_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of number counts to the total number of words in the text for English and German.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Ratio of number counts to the total number of words.\n",
    "    \"\"\"\n",
    "    # Detect the language of the text\n",
    "    language = detect_language(text)\n",
    "\n",
    "    #if language not in ['en', 'de']:\n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Process the text with the appropriate spaCy model\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(text, disable=['ner', 'parser'])\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(text, disable=['ner', 'parser'])\n",
    "    else :\n",
    "        doc = nlp_de(text, disable=['ner', 'parser'])\n",
    "\n",
    "    # Count the number of numeric tokens\n",
    "    number_count = sum(1 for token in doc if token.is_digit)\n",
    "\n",
    "    # Total number of words\n",
    "    total_words = len(doc)\n",
    "\n",
    "    # Calculate the ratio\n",
    "    ratio = (number_count / total_words) * 100 if total_words != 0 else 0\n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e68b85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"number_count_ratio\"] = sentences[\"sentence\"].apply(calculate_number_count_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163bfcf7",
   "metadata": {},
   "source": [
    "#### features : Future Tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "935191d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_future_verb_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the percentage ratio of future tense verbs to the total number of words in the text using spaCy.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    float: Percentage ratio of future tense verbs to the total number of words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Detect the language\n",
    "    language = detect_language(text)\n",
    "\n",
    "    # Process the text with the appropriate spaCy model\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(text, disable=['ner', 'parser'])\n",
    "        future_tense_forms = {'will', 'shall'}\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(text, disable=['ner', 'parser'])\n",
    "        future_tense_forms = {'werde', 'wirst', 'wird', 'werden'}\n",
    "    else:\n",
    "        doc = nlp_de(text, disable=['ner', 'parser'])\n",
    "        future_tense_forms = {'werde', 'wirst', 'wird', 'werden'}\n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Count the number of future tense verbs\n",
    "    future_verb_count = sum(1 for token in doc if token.lemma_ in future_tense_forms and token.pos_ == \"AUX\")\n",
    "\n",
    "    # Total number of words\n",
    "    total_words = len(doc)\n",
    "\n",
    "    # Calculate the ratio\n",
    "    ratio = (future_verb_count / total_words) * 100 if total_words != 0 else 0\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bcfaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"future_verb_ratio\"] = sentences[\"sentence\"].apply(calculate_future_verb_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8333bac",
   "metadata": {},
   "source": [
    "#### features : Past Tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "611c122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_past_verb_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate the percentage ratio of past tense verbs to the total number of words in the text using spaCy.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text.\n",
    "    language (str): Language of the text ('en' for English, 'de' for German).\n",
    "\n",
    "    Returns:\n",
    "    float: Percentage ratio of past tense verbs to the total number of words.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean the text and detect the language\n",
    "    cleaned_text = cleanText(text)\n",
    "    language = detect_language(text)\n",
    "    \n",
    "    # Process the text with the appropriate spaCy model\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(cleaned_text,disable = ['ner', 'parser'])\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(cleaned_text,disable = ['ner', 'parser'])\n",
    "    else:\n",
    "        doc = nlp_de(cleaned_text,disable = ['ner', 'parser'])\n",
    "        #raise ValueError(\"Unsupported language. Supported languages: 'en' (English), 'de' (German)\")\n",
    "\n",
    "    # Count the number of past tense verbs (VERB with past tense morphology)\n",
    "    past_verb_count = sum(1 for token in doc if token.pos_ == 'VERB' and ('VBD'or'VVFIN' in token.tag_))\n",
    "    \n",
    "    # Total number of words\n",
    "    total_words = len(doc)\n",
    "\n",
    "    # Calculate the ratio\n",
    "    ratio = (past_verb_count / total_words) * 100 if total_words != 0 else 0\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd8db219",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"past_verb_ratio\"] = sentences[\"sentence\"].apply(calculate_past_verb_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c52a522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>year</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>sentence_counts</th>\n",
       "      <th>type_token</th>\n",
       "      <th>content_word_diversity</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>average_word_length</th>\n",
       "      <th>spatial_adverbial_ratio</th>\n",
       "      <th>time_adverbial_ratio</th>\n",
       "      <th>modal_verb_ratio</th>\n",
       "      <th>first_person_ratio</th>\n",
       "      <th>impersonal_pronoun_ratio</th>\n",
       "      <th>negation_ratio</th>\n",
       "      <th>exclusive_conjunction_ratio</th>\n",
       "      <th>inclusive_conjunction_ratio</th>\n",
       "      <th>tentative_word_ratio</th>\n",
       "      <th>number_count_ratio</th>\n",
       "      <th>future_verb_ratio</th>\n",
       "      <th>past_verb_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aldi</td>\n",
       "      <td>2020</td>\n",
       "      <td>6283</td>\n",
       "      <td>371</td>\n",
       "      <td>0.178620</td>\n",
       "      <td>0.565418</td>\n",
       "      <td>19.759887</td>\n",
       "      <td>4.683345</td>\n",
       "      <td>0.102951</td>\n",
       "      <td>0.171585</td>\n",
       "      <td>0.645902</td>\n",
       "      <td>0.846849</td>\n",
       "      <td>0.100474</td>\n",
       "      <td>0.358835</td>\n",
       "      <td>0.028707</td>\n",
       "      <td>3.200804</td>\n",
       "      <td>0.043060</td>\n",
       "      <td>5.647287</td>\n",
       "      <td>0.485639</td>\n",
       "      <td>7.664705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aldi</td>\n",
       "      <td>2021</td>\n",
       "      <td>8139</td>\n",
       "      <td>468</td>\n",
       "      <td>0.154746</td>\n",
       "      <td>0.572570</td>\n",
       "      <td>20.411111</td>\n",
       "      <td>4.652041</td>\n",
       "      <td>0.065738</td>\n",
       "      <td>0.065738</td>\n",
       "      <td>0.494234</td>\n",
       "      <td>1.219110</td>\n",
       "      <td>0.076881</td>\n",
       "      <td>0.186711</td>\n",
       "      <td>0.010983</td>\n",
       "      <td>2.877540</td>\n",
       "      <td>0.032949</td>\n",
       "      <td>5.176996</td>\n",
       "      <td>0.276390</td>\n",
       "      <td>7.655135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allianz</td>\n",
       "      <td>2020</td>\n",
       "      <td>61333</td>\n",
       "      <td>2565</td>\n",
       "      <td>0.082896</td>\n",
       "      <td>0.578371</td>\n",
       "      <td>26.288994</td>\n",
       "      <td>5.114752</td>\n",
       "      <td>0.478576</td>\n",
       "      <td>0.162405</td>\n",
       "      <td>0.458408</td>\n",
       "      <td>2.774264</td>\n",
       "      <td>0.324458</td>\n",
       "      <td>0.151811</td>\n",
       "      <td>0.026790</td>\n",
       "      <td>4.226882</td>\n",
       "      <td>0.034232</td>\n",
       "      <td>2.453852</td>\n",
       "      <td>0.220803</td>\n",
       "      <td>10.165354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Allianz</td>\n",
       "      <td>2021</td>\n",
       "      <td>59048</td>\n",
       "      <td>2367</td>\n",
       "      <td>0.082174</td>\n",
       "      <td>0.575771</td>\n",
       "      <td>27.082408</td>\n",
       "      <td>5.089017</td>\n",
       "      <td>0.498630</td>\n",
       "      <td>0.111416</td>\n",
       "      <td>0.469732</td>\n",
       "      <td>2.858832</td>\n",
       "      <td>0.269085</td>\n",
       "      <td>0.183538</td>\n",
       "      <td>0.043551</td>\n",
       "      <td>4.104710</td>\n",
       "      <td>0.048218</td>\n",
       "      <td>2.500385</td>\n",
       "      <td>0.197196</td>\n",
       "      <td>9.651279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Asklepios</td>\n",
       "      <td>2020</td>\n",
       "      <td>12877</td>\n",
       "      <td>737</td>\n",
       "      <td>0.277373</td>\n",
       "      <td>0.828323</td>\n",
       "      <td>19.595598</td>\n",
       "      <td>6.322757</td>\n",
       "      <td>0.514974</td>\n",
       "      <td>0.158453</td>\n",
       "      <td>0.490402</td>\n",
       "      <td>2.494045</td>\n",
       "      <td>0.182149</td>\n",
       "      <td>0.427350</td>\n",
       "      <td>0.112092</td>\n",
       "      <td>3.909206</td>\n",
       "      <td>0.028023</td>\n",
       "      <td>1.378490</td>\n",
       "      <td>1.168568</td>\n",
       "      <td>8.301807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>telekom</td>\n",
       "      <td>2020</td>\n",
       "      <td>66723</td>\n",
       "      <td>3074</td>\n",
       "      <td>0.079060</td>\n",
       "      <td>0.563742</td>\n",
       "      <td>24.523163</td>\n",
       "      <td>4.913692</td>\n",
       "      <td>0.387579</td>\n",
       "      <td>0.169855</td>\n",
       "      <td>0.523400</td>\n",
       "      <td>3.281570</td>\n",
       "      <td>0.438164</td>\n",
       "      <td>0.235730</td>\n",
       "      <td>0.046613</td>\n",
       "      <td>3.332179</td>\n",
       "      <td>0.051940</td>\n",
       "      <td>1.960087</td>\n",
       "      <td>0.133044</td>\n",
       "      <td>9.446501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>telekom</td>\n",
       "      <td>2021</td>\n",
       "      <td>86733</td>\n",
       "      <td>3809</td>\n",
       "      <td>0.069928</td>\n",
       "      <td>0.556177</td>\n",
       "      <td>26.194667</td>\n",
       "      <td>4.825764</td>\n",
       "      <td>0.256932</td>\n",
       "      <td>0.230884</td>\n",
       "      <td>0.385260</td>\n",
       "      <td>3.297784</td>\n",
       "      <td>0.450473</td>\n",
       "      <td>0.208682</td>\n",
       "      <td>0.033108</td>\n",
       "      <td>3.189429</td>\n",
       "      <td>0.036118</td>\n",
       "      <td>1.850748</td>\n",
       "      <td>0.074507</td>\n",
       "      <td>9.111796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>2020</td>\n",
       "      <td>22970</td>\n",
       "      <td>599</td>\n",
       "      <td>0.138063</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>46.541586</td>\n",
       "      <td>4.316807</td>\n",
       "      <td>0.324070</td>\n",
       "      <td>0.115422</td>\n",
       "      <td>0.639429</td>\n",
       "      <td>2.208935</td>\n",
       "      <td>0.535625</td>\n",
       "      <td>0.311410</td>\n",
       "      <td>0.103803</td>\n",
       "      <td>3.504401</td>\n",
       "      <td>0.041521</td>\n",
       "      <td>1.107255</td>\n",
       "      <td>0.235498</td>\n",
       "      <td>8.221226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>2021</td>\n",
       "      <td>20322</td>\n",
       "      <td>532</td>\n",
       "      <td>0.135339</td>\n",
       "      <td>0.578111</td>\n",
       "      <td>48.993103</td>\n",
       "      <td>4.206175</td>\n",
       "      <td>0.324854</td>\n",
       "      <td>0.159928</td>\n",
       "      <td>0.643072</td>\n",
       "      <td>1.830642</td>\n",
       "      <td>0.492865</td>\n",
       "      <td>0.276943</td>\n",
       "      <td>0.098573</td>\n",
       "      <td>3.370259</td>\n",
       "      <td>0.028164</td>\n",
       "      <td>1.107011</td>\n",
       "      <td>0.233547</td>\n",
       "      <td>8.646264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>vonovia</td>\n",
       "      <td>2020</td>\n",
       "      <td>51533</td>\n",
       "      <td>2287</td>\n",
       "      <td>0.085656</td>\n",
       "      <td>0.560636</td>\n",
       "      <td>25.406795</td>\n",
       "      <td>4.952638</td>\n",
       "      <td>0.381002</td>\n",
       "      <td>0.115697</td>\n",
       "      <td>0.571169</td>\n",
       "      <td>3.282039</td>\n",
       "      <td>0.368553</td>\n",
       "      <td>0.267244</td>\n",
       "      <td>0.052401</td>\n",
       "      <td>3.627884</td>\n",
       "      <td>0.038427</td>\n",
       "      <td>1.790410</td>\n",
       "      <td>0.233532</td>\n",
       "      <td>9.222546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        company  year  word_counts  sentence_counts  type_token  \\\n",
       "0          Aldi  2020         6283              371    0.178620   \n",
       "1          Aldi  2021         8139              468    0.154746   \n",
       "2       Allianz  2020        61333             2565    0.082896   \n",
       "3       Allianz  2021        59048             2367    0.082174   \n",
       "4     Asklepios  2020        12877              737    0.277373   \n",
       "..          ...   ...          ...              ...         ...   \n",
       "115     telekom  2020        66723             3074    0.079060   \n",
       "116     telekom  2021        86733             3809    0.069928   \n",
       "117  volkswagen  2020        22970              599    0.138063   \n",
       "118  volkswagen  2021        20322              532    0.135339   \n",
       "119     vonovia  2020        51533             2287    0.085656   \n",
       "\n",
       "     content_word_diversity  average_sentence_length  average_word_length  \\\n",
       "0                  0.565418                19.759887             4.683345   \n",
       "1                  0.572570                20.411111             4.652041   \n",
       "2                  0.578371                26.288994             5.114752   \n",
       "3                  0.575771                27.082408             5.089017   \n",
       "4                  0.828323                19.595598             6.322757   \n",
       "..                      ...                      ...                  ...   \n",
       "115                0.563742                24.523163             4.913692   \n",
       "116                0.556177                26.194667             4.825764   \n",
       "117                0.566038                46.541586             4.316807   \n",
       "118                0.578111                48.993103             4.206175   \n",
       "119                0.560636                25.406795             4.952638   \n",
       "\n",
       "     spatial_adverbial_ratio  time_adverbial_ratio  modal_verb_ratio  \\\n",
       "0                   0.102951              0.171585          0.645902   \n",
       "1                   0.065738              0.065738          0.494234   \n",
       "2                   0.478576              0.162405          0.458408   \n",
       "3                   0.498630              0.111416          0.469732   \n",
       "4                   0.514974              0.158453          0.490402   \n",
       "..                       ...                   ...               ...   \n",
       "115                 0.387579              0.169855          0.523400   \n",
       "116                 0.256932              0.230884          0.385260   \n",
       "117                 0.324070              0.115422          0.639429   \n",
       "118                 0.324854              0.159928          0.643072   \n",
       "119                 0.381002              0.115697          0.571169   \n",
       "\n",
       "     first_person_ratio  impersonal_pronoun_ratio  negation_ratio  \\\n",
       "0              0.846849                  0.100474        0.358835   \n",
       "1              1.219110                  0.076881        0.186711   \n",
       "2              2.774264                  0.324458        0.151811   \n",
       "3              2.858832                  0.269085        0.183538   \n",
       "4              2.494045                  0.182149        0.427350   \n",
       "..                  ...                       ...             ...   \n",
       "115            3.281570                  0.438164        0.235730   \n",
       "116            3.297784                  0.450473        0.208682   \n",
       "117            2.208935                  0.535625        0.311410   \n",
       "118            1.830642                  0.492865        0.276943   \n",
       "119            3.282039                  0.368553        0.267244   \n",
       "\n",
       "     exclusive_conjunction_ratio  inclusive_conjunction_ratio  \\\n",
       "0                       0.028707                     3.200804   \n",
       "1                       0.010983                     2.877540   \n",
       "2                       0.026790                     4.226882   \n",
       "3                       0.043551                     4.104710   \n",
       "4                       0.112092                     3.909206   \n",
       "..                           ...                          ...   \n",
       "115                     0.046613                     3.332179   \n",
       "116                     0.033108                     3.189429   \n",
       "117                     0.103803                     3.504401   \n",
       "118                     0.098573                     3.370259   \n",
       "119                     0.052401                     3.627884   \n",
       "\n",
       "     tentative_word_ratio  number_count_ratio  future_verb_ratio  \\\n",
       "0                0.043060            5.647287           0.485639   \n",
       "1                0.032949            5.176996           0.276390   \n",
       "2                0.034232            2.453852           0.220803   \n",
       "3                0.048218            2.500385           0.197196   \n",
       "4                0.028023            1.378490           1.168568   \n",
       "..                    ...                 ...                ...   \n",
       "115              0.051940            1.960087           0.133044   \n",
       "116              0.036118            1.850748           0.074507   \n",
       "117              0.041521            1.107255           0.235498   \n",
       "118              0.028164            1.107011           0.233547   \n",
       "119              0.038427            1.790410           0.233532   \n",
       "\n",
       "     past_verb_ratio  \n",
       "0           7.664705  \n",
       "1           7.655135  \n",
       "2          10.165354  \n",
       "3           9.651279  \n",
       "4           8.301807  \n",
       "..               ...  \n",
       "115         9.446501  \n",
       "116         9.111796  \n",
       "117         8.221226  \n",
       "118         8.646264  \n",
       "119         9.222546  \n",
       "\n",
       "[120 rows x 20 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15501eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv(\"myfeatures.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
